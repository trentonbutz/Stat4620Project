---
title: "Stat 4620 Final Project Initial EDA"
author: "Nicolas Brizzi"
date: "2022-11-28"
output: html_document
---

```{r}
library(dplyr)
library(magrittr)
library(tidyr)
library(GGally)
library(MASS)
library(pls)
library(ggplot2)
library(scales)
```

Reading in data
```{r}
train = read.csv("/Users/Nick/Downloads/Ames/train.csv")
test = read.csv("/Users/Nick/Downloads/Ames/test_new.csv")
```

## EDA 

Frequency of data types in the Ames frame:
```{r}
table(sapply(train, class))
```
About half and half - slightly more character data types. 

Quick data summaries.
```{r}
summary(train)
```

How much data is missing?
```{r}
sum(is.na(train))
```
6965 different values are NA

Let's replace those NA values with 0 or None depending on the variable type. 
```{r}
for (i in 1:length(names(train))) {
  if(class(train[[i]]) == "character") {
    train[[i]] <- train[[i]] %>% replace_na("None")
  }
  else {
    train[[i]] <- train[[i]] %>% replace_na(0)
  }
}

for (i in 1:length(names(test))) {
  if(class(test[[i]]) == "character") {
    test[[i]] <- test[[i]] %>% replace_na("None")
  }
  else {
    test[[i]] <- test[[i]] %>% replace_na(0)
  }
}
```

```{r}
sum(is.na(train))
sum(is.na(test))
```

### Seeing if there is any linear relationships between some of the numeric predictors that we can pick up on. 

Looking
```{r}
train %>%
  dplyr::select(contains("Garage")) %>%
  select_if(is.numeric) %>%
  ggpairs()
```
It looks like 'GarageCars' and 'GarageArea' have a very high linear correlation, which is to be expected. 
The other garage variables have moderately strong linear correlations amongst themselves. Perhaps we only keep 'GarageArea'.

Variables that contain "Overall"
```{r}
train %>%
  dplyr::select(contains("Overall")) %>%
  select_if(is.numeric) %>%
  ggpairs()
```

How much can Overall Quality tell us about the house sale price?
```{r}
lm(SalePrice ~ OverallQual, data = train) %>%
  summary()
```
R-squared of ~.63, decently strong correlation for just one variable. This most likely will be included in the model that we end up on. 


Variables that contain "SF" or Square Feet
```{r}
train %>%
  dplyr::select(contains("SF")) %>%
  select_if(is.numeric) %>%
  ggpairs()
```
There's a lot going on but there are some strong correlations here. 
- '1stFlrSF' and 'TotalBsmtSF' have a correlation of .820
- 'BsmtFinSF1' and 'TotalBsmtSF' have a correlation of .522

All other correlations are below .5. 

## Initial Considerations

So far there is a lot of character variables that can be interpreted as ordinal, and a lot of numeric variables. These variables are good for techniques that have a lot of quantitative variables that have complex, and perhaps non-linear, relationships among each other. It can potentially eliminate a lot of factors and retain an accurate model. 

It will be important to decide on what factors to convert into numeric values, and what variables to exclude from the model as a whole. 

```{r}
train %>%
  dplyr::select(-BsmtFinSF1,-X1stFlrSF, -GarageCars, -GarageYrBlt)
```

###Initial PCR model
```{r}
set.seed(2048)
pcr.fit = pcr(SalePrice ~., data = train %>%
                dplyr::select(-BsmtFinSF1,-X1stFlrSF, -GarageCars, -GarageYrBlt) %>%
                select_if(is.numeric),
              scale = TRUE, validation = "CV")
summary(pcr.fit)
```
70% of the variation in SalePrice is explained by the first component. It plateaus quickly after that. It would be productive to move over to PLS to get the components that are explaining SalePrice and X, so we only need the first 5-10 components to get up to 80% variability explained in Y, by my estimations. 

Looking at cross validation plot of Mean Squared Error by # of Components. Barely changes after the first couple of components.
```{r}
validationplot(pcr.fit, val.type = "MSEP")
```

First PLS model
```{r}
set.seed(196)
pls.fit = plsr(SalePrice ~., data = train %>%
                dplyr::select(-BsmtFinSF1,-X1stFlrSF, -GarageCars, -GarageYrBlt) %>%
                select_if(is.numeric),
              scale = TRUE, validation = "CV")
summary(pls.fit)
```
First 3 components explain 80.13% of the variation in Y, and it levels off after that. 4-6 would be the most that would be worth considering. It looks like the lowest cross-validation error occurs at 6: 38,315


Looking at cross validation plot of Mean Squared Error by # of Components.
```{r}
validationplot(pls.fit, val.type = "MSEP")
```

Lets do prediction with the 6 component model.

```{r}
pls.pred = predict(pls.fit, test, ncomp = 6)

mean((pls.pred - test$SalePrice)^2)

sqrt(mean((pls.pred - test$SalePrice)^2))
```
The prediction is off by an average of $31,407

Comparing 31k against the distribution of 'SalePrice' shown below, it's not too bad of a model.
```{r}
ggplot(data = test, aes(x=SalePrice)) + geom_boxplot()
```


Lets see what we get from other components

Graphing:
```{r}
for (i in 1:33){
  y[i] = mean((predict(pls.fit, test, ncomp = i) - test$SalePrice)^2)
}
x = seq(1,33)

df = data.frame(x = x, y = y)
ggplot(data = df, aes(x = x, y = y)) + 
  geom_line() + 
  geom_point() + 
  labs(title = "MSE by Components", x = "# of Components", y = "MSE")
```
5 components might be a little better here. 

```{r}
mean((predict(pls.fit, test, ncomp = 5) - test$SalePrice)^2)
```
Slightly better MSE, with an average sale price deviation of $31,250

```{r}
df = data.frame(y = test$SalePrice, x = pls.pred[,1,1])

ggplot(data = df, aes(x = x, y = y)) + geom_point() + 
  labs(x = "Predicted Sale Price", y = "Actual Sale Price") + 
  geom_abline(slope = 1)
```
There appears to be somewhat of a non-linear relationship at play here. Maybe if we tried to predict the log of the sale price, we could get better results. Besides that, the model is doing a pretty good job.


PLS model on the log of Sale Price.
```{r}
set.seed(196)
log.pls.fit = plsr(log(SalePrice) ~., data = train %>%
                dplyr::select(-BsmtFinSF1,-X1stFlrSF, -GarageCars, -GarageYrBlt) %>%
                select_if(is.numeric),
              scale = TRUE, validation = "CV")
summary(log.pls.fit)
```
It looks like the new model is doing a better job of explaining the variance in the response variable. Within 5 components, we are at 86.4%. It looks like the cross validation is telling us to pick 4 components here. Which is more concise than before. 

```{r}
#Taking the exponentiated predictions as the new prediction matrix
log.pls.pred = exp(predict(log.pls.fit, test, ncomp = 4))

mean((log.pls.pred - test$SalePrice)^2)

sqrt(mean((log.pls.pred - test$SalePrice)^2))
```
It appears that the MSE is slightly worse than before. Let's look at the data.


```{r}
log.df = data.frame(y = test$SalePrice, x = log.pls.pred[,1,1])

ggplot(data = log.df, aes(x = x, y = y)) + geom_point() + 
  labs(x = "Predicted Sale Price", y = "Actual Sale Price") + 
  geom_abline(slope = 1)
```
This new relationship appears to be less patterned than before. Although the model appears to still underestimate the more expensive houses. Overall, this looks better, if we don't weigh the effects of the outlier too heavily. 


What if we removed the outlier?
```{r}
test.removed = test[-1082,]

log.pls.pred.out.removed = exp(predict(log.pls.fit, test.removed, ncomp = 4))

mean((log.pls.pred.out.removed - test.removed$SalePrice)^2)

sqrt(mean((log.pls.pred.out.removed - test.removed$SalePrice)^2))
```
We see that we get a new MSE of 644 million! Which is the best result we have seen up until this point, but we had to remove something to achieve it.


Plotting the new graph with the removed point.
```{r}
log.df.removed = data.frame(y = test.removed$SalePrice, x = log.pls.pred.out.removed[,1,1])

ggplot(data = log.df.removed, aes(x = x, y = y)) + geom_point() + 
  labs(x = "Predicted Sale Price", y = "Actual Sale Price", title = "Partial Least Squares on Log of Sale Price Model") + 
  geom_abline(slope = 1)
```

Residual plot
```{r}
resids = test.removed$SalePrice - log.pls.pred.out.removed[,1,1]

resids.df = data.frame(x = log.pls.pred.out.removed[,1,1], y = resids)

ggplot(data = resids.df, aes(x = log.pls.pred.out.removed[,1,1], y = resids)) + 
  geom_point() + labs(x = "Predicted Sale Price", y = "Residuals", title = "Partial Least Squares on Log of Sale Price Model") +
  geom_abline(slope = 0) + scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma)
```



