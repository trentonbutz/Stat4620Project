---
title: "Stat 4620 Project Code"
author: "Trenton Butz"
date: '2022-11-22'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First, let's import the data.

```{r}
library(tidyverse)
library(readr)
train <- read_csv("~/OSU Classes/Stat 4620/Ames/Ames/train.csv")
```

```{r}
test <- read_csv("~/OSU Classes/Stat 4620/Ames/Ames/train.csv")
```


Let's take a look at the first five rows of the dataset to get a sense of its structure.

```{r}
head(train, n = 5)
```

It looks like we have a fair mix of categorical and continuous predictor variables in the data. It also looks like many variables are interrelated, which could mean there is multicollinearity in the data. This might suggest that it would be helpful to use a dimension reduction technique such as PCA or PLS, but more on that later. 

Now, we can use `sapply()` to check whether or not there is any missing data to deal with. 

```{r}
sapply(train, function(x) sum(is.na(x)))
```

```{r}
is.numeric(train$LotFrontage)
class(train$PoolQC)
class(train[[2]])
```

```{r}
for (i in 1:length(names(train))) {
  if(class(train[[i]]) == "character") {
    train[[i]] <- train[[i]] %>% replace_na("None")
  }
  else {
    train[[i]] <- train[[i]] %>% replace_na(0)
  }
}

for (i in 1:length(names(test))) {
  if(class(test[[i]]) == "character") {
    test[[i]] <- test[[i]] %>% replace_na("None")
  }
  else {
    test[[i]] <- test[[i]] %>% replace_na(0)
  }
}
```


Let's check the multicollinearity in a set of seemingly related columns. 

```{r}
pairs(data.frame(train$SalePrice, train$TotalBsmtSF, train$`1stFlrSF`, train$`2ndFlrSF`, train$GarageArea))
```

As expected, the variables which contain information about square footage all seem to be somewhat correlated with each other and with `SalePrice`. For example, basement square footage is very positively correlated with 1st floor square footage, which makes sense because if a house has a large basement, it probably has a proportionally large 1st floor. Let's get an exact numerical summary of the extent to which these variables are correlated.  

```{r}
cor(data.frame(train$SalePrice, train$TotalBsmtSF, train$`1stFlrSF`, train$`2ndFlrSF`, train$GarageArea))
```

We see that Sale Price has a correlation of approximately 0.6 with three of the four predictors we close arbitrarily, which indicates a moderate to strong positive correlation. Overall, many of the correlations here are moderate to strong. We should consider some methods of dimension reduction to address this problem. One approach would be to manually identify the variables in the data which might be correlated and choose one to be a representative of the group for our model, as we've started to do above. However, this is tedious, and would really not work well in cases where p>n. Instead, we should try the dimension reduction methods that we learned in class, such as PCA and PLS. We could also try LASSO and Ridge regression to shrink the predictors toward 0 (and make some =0 in the case of LASSO). 

Let's try a Principal Component Analysis, starting with all of the predictors in the dataset with the goal of predicting `SalePrice`. 

```{r}
if(!require('pls')){
  install.packages('pls')
}
if(!require('ISLR')){
  install.packages('ISLR')
}
library(pls)
library(ISLR)
```

We need to either convert our factor variables to numeric or drop them entirely. We will try both. 

```{r}
# tries to convert columns to numeric
# train[sapply(train, is.character)] <- data.matrix(train[sapply(train, is.character)])
```

```{r}
pcr_train <- train[, !sapply(train, is.character)]

pcr_test <- test[, !sapply(test, is.character)]
```

Fitting the model

```{r}
set.seed(10)  # only if you want to exactly repeat this output.
pcr.fit = pcr(SalePrice~., data=pcr_train, scale=TRUE, validation="CV")
```

Printing model summary

```{r}
summary(pcr.fit)
```

This is pretty busy. Let's see what number of principal components is the best. 

```{r}
validationplot(pcr.fit,val.type="MSEP")
```

From the summary and the plot, the best performance is attained when there are 31 principal components. This is not great, since the max was 37. Because of the fact that PCR limits interpretability and we're only getting a small reduction in the dimensionality of our data, there are probably better approaches out there. 

Continuing to fit the model with 31 principal components:

```{r}
pcr.fit2 = pcr(SalePrice~., data=pcr_train, scale=TRUE, ncomp=31)

pcr.pred = predict(pcr.fit2,ncomp=31)

mean((as.vector(pcr.pred)-test$SalePrice)^2)
```

CV error is 1.177 billion



