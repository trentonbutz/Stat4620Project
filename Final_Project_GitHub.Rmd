---
title: "Using Statistical Learning to Predict Sale Price of Homes in Ames, Iowa"
author: "AJ Turner, Chris Holman, Patrick Tiernan, Nick Brizzi, Trenton Butz"
date: '2022-12-06'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

## Section I: Exploratory Data Analysis

Before we are able to perform Exploratory Data Analysis and investigate the Ames Housing Data, we must first load in the training and testing data as well as any packages that will be required in our analysis.

```{r}
# If you have not installed any of the below packages, please run 
# install.packages('package') in your R console, replacing "package"
# with the name of the package which you have not installed. 
library(tidyverse)
library(readr)
library(ggplot2)
library(glmnet)
library(pls)
```

```{r}
# Change the file paths in the following code to match the locations in 
# which you've stored the training and testing datasets, respectively.
test = read.csv("/Users/Nick/Downloads/Ames/test_new.csv")
train = read.csv("/Users/Nick/Downloads/Ames/train.csv")
```

First, let's take a look at the first five rows of the training dataset to get a sense of its structure.

```{r}
head(train, n = 5)
```

From this, we can tell that there are 81 variables in the training data. Of these, there appears to be some mixture of quantitative variables and categorical variables, which will inform our choice of statistical learning techniques. We should also note that the quantitative variables appear to exist on rather different scales, which means that we may which to choose techniques that perform variable standardization (or perform this standardization manually) to ensure that the predictor variables all have equitable contributions in our models.

As we're examining potential problems with our dataset, we should consider the impact that missing data will have on the execution of our model. Many R functions automatically exclude missing data from our analysis. This may be appropriate if there is not a lot of missing data, but if NAs are a more widespread problem, we may choose to interpolate specific values instead.

```{r}
head(sapply(train, function(x) sum(is.na(x))), n =10)
```

From this output, we see that there are 259 missing values in the `LotFrontage` variable, as well as 1,369 missing values in the `Alley` variable - and that's just the missing values in the first ten columns of the training data set. Since we only have approximately 1,400 observations in the first place, we certainly cannot exclude all instances which have an NA in any column. A better method will be interpolation, which will look slightly different for categorical variables than for quantitative variables. For character variables, we will choose to set NA instances to "None" in order to keep the new values consistent with the existing values within the same column. For quantitative variables, we have chosen to set missing values to 0. It is also possible to use the column mean, but take the variable LotFrontage as an example: an NA value may attempt to convey that the property is an apartment complex or some similar building with zero distance to the street, so replacing NA with the column mean would be inaccurate.

```{r}
for (i in 1:length(names(train))) {
  if(class(train[[i]]) == "character") {
    train[[i]] <- train[[i]] %>% replace_na("None")
  }
  else {
    train[[i]] <- train[[i]] %>% replace_na(0)
  }
}
for (i in 1:length(names(test))) {
  if(class(test[[i]]) == "character") {
    test[[i]] <- test[[i]] %>% replace_na("None")
  }
  else {
    test[[i]] <- test[[i]] %>% replace_na(0)
  }
}
```

Missing data is not the only problem that we are facing with our data. It would be smart to check the columns in each data set more closely to make sure that all of the columns are truly equal between the training and the testing data. This will help make sure that no errors arise when we get to the prediction stage of our analysis.

```{r}
for (i in 1:length(train))
  if (colnames(train)[i] != colnames(test)[i]){
    print(colnames(train)[i])
    print(colnames(test)[i]) 
  }
```

In fact, there are three variables whose names differ between the training and the testing data. The pattern appears to be that in the testing data, variables whose names start with a number have been given a leading X, while the same is not true for the training data. This could be due to R automatically "fixing" the column names. Either way, we should fix it.

```{r}
colnames(train)[which(names(train) == "X1stFlrSF")] <- "1stFlrSF"
colnames(train)[which(names(train) == "X2ndFlrSF")] <- "2ndFlrSF"
colnames(train)[which(names(train) == "X3SsnPorch")] <- "3SsnPorch"

colnames(test)[which(names(test) == "X1stFlrSF")] <- "1stFlrSF"
colnames(test)[which(names(test) == "X2ndFlrSF")] <- "2ndFlrSF"
colnames(test)[which(names(test) == "X3SsnPorch")] <- "3SsnPorch"
```

One way to begin searching for collinearity among the predictor variables is to find the variables which are named similarly and check if they are collinear. For instance, one can find a handful of variables in the training data set whose named end in the string `SF`. This indicates "square footage", or the length of a region in feet multiplied by its width, in feet. It makes sense that there are multiple variables indicating square footage because it might be important to find the square footage of each room in a given house. Let's take a look at a pairs plot involving several of these variables.

```{r}
pairs(data.frame(train$TotalBsmtSF, train$`1stFlrSF`, train$`2ndFlrSF`, train$GarageArea), main = "Correlation of Variables related to Area")
```

It turns out that there is a significant amount of collinearity between most of these variables, as we expected. For example, Total Basement Square Footage appears to have a strong positive correlation with First Floor Square Footage, which makes sense intuitively. The size of the basement appears to be more moderately correlated with the area of the garage, which also seems reasonable given that a homeowner's decision to fit one more car in their garage has less of a direct impact on how large of a basement they are interested in.

Several variables which include the word "Garage" are similarly collinear. The following small correlation matrix shows that the variable that encodes the area of a garage has a strong positive correlation with the variable that encodes the number of cars that the garage is designed to contain.

```{r}
cor(data.frame(train$GarageCars, train$GarageArea))
```

We can take a similar approach to make an early determination of which variables are likely to be important in predicting Sale Price. Based on our previous knowledge, we anticipate that the size of the house, as well as the neighborhood that the house is in, will be most effective at predicting the sale price of that house. We can check the first assumption with another pairs plot, and we can check the second assumption with a boxplot, since `Neighborhood` is a categorical variable. Since there are many neighborhoods in the data and we don't want to end up with a crowded plot, we will only plot neighborhoods that occur in the training data more than 50 times.

```{r}
par(mfrow = c(2,1))
pairs(data.frame(train$SalePrice, train$TotalBsmtSF, train$`1stFlrSF`, train$`2ndFlrSF`), main = "Correlation of Sale Price with Square Footage Predictors")
neighborhoods <- train %>% 
  group_by(Neighborhood) %>% 
  count(Neighborhood) %>% 
  filter(n > 50) %>% 
  ungroup()
train %>% 
  filter(Neighborhood %in% neighborhoods$Neighborhood) %>% 
  ggplot() + 
    geom_boxplot(aes(x = Neighborhood, y = SalePrice)) + 
    labs(title = "Effect of Neighborhood on Sale Price")
#boxplot(train$SalePrice ~ train$Neighborhood, main = "Correlation of Sale Price with Neighborhood", xlab = "Name of Neighborhood", ylab = "Sale Price ($)")
```

In the first plot, we can look more closely at the first column to gauge the correlation of each of the three chosen predictors on Sale Price. All three predictors seem to be positively correlated with the dependent variable, and all three are at least moderately correlated with `SalePrice`. This means that our intuition that the size of a house is predictive of the price for which it will be sold is correct. Looking more closely at the second plot, it appears that the neighborhood encoded "NRidgHt" has the highest median SalePrice at nearly two times that of many other prevalent neighborhoods. When it comes to interpreting the results of our model, we will likely see that a house in Northridge Heights is predicted to sell for a lot more money than another house, all else equal.

### Chris: include key figures, discuss modeling approaches,
### review existing EDA and add other parts as you see fit.
### add boxplots for cate vars that are useful and talk about how we don't wanna get rid of them
### add modeling selection walkthrough

```{r}
numeric_cols <- unlist(lapply(train, is.numeric))
numer_data <- train[,numeric_cols]
cm <- cor(numer_data)[38,]
strong <- abs(cm) > 0.5 & abs(cm) < 1
cm[strong]
```
Once we've gotten a feel for the dataset and cleaned it up, its important to make sure we understand the variables. Above shows numerical variable's with high correlation (above 0.6) with our dependent variable of Sale Price. The larger the magnitude of a variables correlation, the more predictive power it can add to the model. These are the predictors that we should take some time to understand because most likely they will be used in the final model. Below is a brief summary of what these predictors represent.

* OverallQual: Overall Quality quantifies the houses overall material and finish of the house from 1-10  
* TotalBsmtSF, 1stFlrSF & GrLivArea: Square footage of the entire basement, first floor, and all  living area above ground respectively * YearBuilt & YearRemodAdd: The year that the house was built or remodeled respectively
* FullBath: Quantifies the number of full bathrooms above ground
* TotRmsAbvGrd: Quantifies the number of rooms above ground in the house.
* GarageCars & GarageArea: Shows the number of cars and Sqaure Footage of the garage respectively  
    
  
Now while all of these variables show moderate to strong relationships with the dependent variable, they also overlap quite a bit. Meaning some of these predictors represent that same thing as others. So variables such as GarageCars and GarageArea are both contributing almost the same information to the model. This is something important to look out for when fitting models.






## Section II: Model Analysis

### Lasso Model

We decided to try a lasso model because it is capable of using both quantitative and categorical data. Lasso regression uses shrinkage by adding a penalty term to the cost function. This causes some of the coefficients in the model to be exactly zero, eliminating those features from the model. This is useful because it can reduce the complexity of the model and prevent overfitting.

First, we had to set up the data to be used in our Lasso model. We ran into an issue where the train set had a few colums that were missing from the test set and vice versa. To solve this, we dropped the columns in each set that were missing in the other. This solution was used due to some analysis on the variables not included in both the training and test data. Most of these entries were 'None' variables meaning that the information was simply not in the data set. An example of this is 'SaleTypeNone'. Other examples were ultra-specific category types, such as 'ElectricalMix' that would not be easily interpretable anyways. So, we decided the best course of action would be to simply eliminate these uncommon categorical variables.

```{r,echo=FALSE, results=FALSE}
# Set train data
train_y = train$SalePrice
train_x = model.matrix(SalePrice~., train)[, -1]
# Set test data
test_y = test$SalePrice
test_x = model.matrix(SalePrice~., test)[, -1]
# Check for missing columns in each and fix
missing_test = setdiff(colnames(train_x),colnames(test_x))
missing_train = setdiff(colnames(test_x),colnames(train_x))
train_x = train_x[,!colnames(train_x) %in% setdiff(colnames(train_x), colnames(test_x))]
test_x = test_x[,!colnames(test_x) %in% setdiff(colnames(test_x), colnames(train_x))]
```

Next, we used cross validation to determine the ideal value of lambda for our Lasso model. The minimizing lambda value ended up being 1149.997.

```{r,echo=FALSE}
set.seed(101)
lasso_cv = cv.glmnet(train_x,train_y,alpha=1)
plot(lasso_cv)
plot(lasso_cv$lambda,lasso_cv$cvm,xlim=c(0,100000), xlab = expression(lambda), ylab = 'CV')
lambda_cv = lasso_cv$lambda.min 
```

We used this value of lambda to fit the Lasso model below.

```{r, echo=FALSE}
fit_lasso = glmnet(train_x,train_y,alpha=1, lambda=lambda_cv)
coef(fit_lasso)
nono_zero_coeff <- sum(fit_lasso$beta != 0)
```

Another benefit of using a Lasso model is that it can be easily interpreted. Since the model completely zeroes out features that aren't significant in making predictions, we are left with only those features that significantly impact our predictions. Our model ended up with a total of 76 significant features. These selected features contain a fairly even mix of continuous and categorical variables. As expected, the continuous variables scaling the typical factors in home price are included, such as the overall quality, overall condition, and year built. These were the factors we expected to be impactful on the sale price. Furthermore, we attempted PLS with the thought that some of the categorical variables may be accounted for in these continuous ones. However, this proved to not be the case as certain categorical variables were impactful in this Lasso model. Some of these include certain neighborhoods, building types, and exterior finish types. This goes to show that there are truly many factors involved in modeling the price of the sale of a house and also shows that all this information is simply unable to be captured from a simply one to ten quality or condition rating.      
  
This produced a test MSE of 743,357,226. We did, however, notice on the plot that there appears to be one point that is an outlier. There is one point that has a squared error of 165,443,629,151 which is 105,709,376,321 greater than the second greatest squared error. The plot below displays this with the red point in the bottom right of the plot. If this outlier is taken out of the test data, the MSE drops to 629,456,623. This is a significant difference simply outlined by the removal of a single point. The data points appear to show a strong linear trend with an $R^2$ value of .8868. With the removal of the outlier, the $R^2$ value rises to .9065. Hence, the model explains about 90% of the variance in the test data with or without the outlier. This is a very strong correlation and, hence, we concluded this to be our best model.  

```{r, echo=FALSE, results=FALSE}
pred_lasso = predict(fit_lasso,newx=test_x)
mean((test_y-pred_lasso)^2)
```

```{r,echo=FALSE}
errors_w_outlier = (test_y-pred_lasso)^2
mse_outliers <- mean(errors_w_outlier)
errors_no_outlier = errors_w_outlier[errors_w_outlier != max(errors_w_outlier)]
errors_no_outlier_mean = mean(errors_no_outlier)
mse_nooutlier <- errors_no_outlier_mean
pred_v_real <- data.frame(x = pred_lasso, y = test_y)
highlight_df <- pred_v_real %>% 
             filter(pred_lasso > 550000)
no_outlier <- pred_v_real %>% 
             filter(pred_lasso < 550000)
pred_v_real %>% ggplot(aes(x=pred_lasso, y=test_y)) +
    geom_point() +
    geom_point(data = highlight_df, aes(x=s0, y=y), color = 'red') +
    geom_smooth(method=lm) + 
  xlab('Predicted Sale Price') +
  ylab('Actual Sale Price')
# find r^2 with outlier
r2_outlier <- summary(lm(test_y ~ pred_lasso))
# r^2 with no outlier
r2_nooutlier <- summary(lm(y ~ s0, data = no_outlier))
```

PCR and PLS models:

## Initial Considerations

So far there is a lot of character variables that can be interpreted as ordinal, and a lot of numeric variables. These variables are good for techniques that have a lot of quantitative variables that have complex, and perhaps non-linear, relationships among each other. It can potentially eliminate a lot of factors and retain an accurate model. 

It will be important to decide on what factors to convert into numeric values, and what variables to exclude from the model as a whole. 

```{r}
train %>%
  dplyr::select(-BsmtFinSF1,-`1stFlrSF`, -GarageCars, -GarageYrBlt)
```

###Initial PCR model
```{r}
set.seed(2048)
pcr.fit = pcr(SalePrice ~., data = train %>%
                dplyr::select(-BsmtFinSF1,-`1stFlrSF`, -GarageCars, -GarageYrBlt) %>%
                select_if(is.numeric),
              scale = TRUE, validation = "CV")
summary(pcr.fit)
```
70% of the variation in SalePrice is explained by the first component. It plateaus quickly after that. It would be productive to move over to PLS to get the components that are explaining SalePrice and X, so we only need the first 5-10 components to get up to 80% variability explained in Y, by my estimations. 

Looking at cross validation plot of Mean Squared Error by # of Components. Barely changes after the first couple of components.
```{r}
validationplot(pcr.fit, val.type = "MSEP")
```

First PLS model
```{r}
set.seed(196)
pls.fit = plsr(SalePrice ~., data = train %>%
                dplyr::select(-BsmtFinSF1,-`1stFlrSF`, -GarageCars, -GarageYrBlt, -`2ndFlrSF`) %>%
                select_if(is.numeric),
              scale = TRUE, validation = "CV") 
summary(pls.fit)
```
First 3 components explain 80.13% of the variation in Y, and it levels off after that. 4-6 would be the most that would be worth considering. It looks like the lowest cross-validation error occurs at 6: 38,315


Looking at cross validation plot of Mean Squared Error by # of Components.
```{r}
validationplot(pls.fit, val.type = "MSEP")
```

Lets do prediction with the 6 component model.

```{r}
pls.pred = predict(pls.fit, test, ncomp = 6)

mean((pls.pred - test$SalePrice)^2)

sqrt(mean((pls.pred - test$SalePrice)^2))
```
The prediction is off by an average of $31,478

Comparing 31k against the distribution of 'SalePrice' shown below, it's not too bad of a model.
```{r}
ggplot(data = test, aes(x=SalePrice)) + geom_boxplot()
```


5 components might be a little better here. 

```{r}
mean((predict(pls.fit, test, ncomp = 4) - test$SalePrice)^2)
mean((predict(pls.fit, test, ncomp = 5) - test$SalePrice)^2)
mean((predict(pls.fit, test, ncomp = 6) - test$SalePrice)^2)
```
Slightly better MSE, with an average sale price deviation of $31,250

```{r}
df = data.frame(y = test$SalePrice, x = pls.pred[,1,1])

ggplot(data = df, aes(x = x, y = y)) + geom_point() + 
  labs(x = "Predicted Sale Price", y = "Actual Sale Price") + 
  geom_abline(slope = 1)
```
There appears to be somewhat of a non-linear relationship at play here. Maybe if we tried to predict the log of the sale price, we could get better results. Besides that, the model is doing a pretty good job.


PLS model on the log of Sale Price.
```{r}
set.seed(196)
log.pls.fit = plsr(log(SalePrice) ~., data = train %>%
                dplyr::select(-BsmtFinSF1,-`1stFlrSF`, -GarageCars, -GarageYrBlt) %>%
                select_if(is.numeric),
              scale = TRUE, validation = "CV")
summary(log.pls.fit)
```
It looks like the new model is doing a better job of explaining the variance in the response variable. Within 5 components, we are at 86.4%. It looks like the cross validation is telling us to pick 4 components here. Which is more concise than before. 

```{r}
#Taking the exponentiated predictions as the new prediction matrix
log.pls.pred = exp(predict(log.pls.fit, test, ncomp = 4))

mean((log.pls.pred - test$SalePrice)^2)

sqrt(mean((log.pls.pred - test$SalePrice)^2))
```
It appears that the MSE is slightly worse than before. Let's look at the data.


```{r}
log.df = data.frame(y = test$SalePrice, x = log.pls.pred[,1,1])

ggplot(data = log.df, aes(x = x, y = y)) + geom_point() + 
  labs(x = "Predicted Sale Price", y = "Actual Sale Price") + 
  geom_abline(slope = 1)
```
This new relationship appears to be less patterned than before. Although the model appears to still underestimate the more expensive houses. Overall, this looks better, if we don't weigh the effects of the outlier too heavily. 


What if we removed the outlier?
```{r}
test.removed = test[-1082,]

log.pls.pred.out.removed = exp(predict(log.pls.fit, test.removed, ncomp = 4))

mean((log.pls.pred.out.removed - test.removed$SalePrice)^2)

sqrt(mean((log.pls.pred.out.removed - test.removed$SalePrice)^2))
```
We see that we get a new MSE of 644 million! Which is the best result we have seen up until this point, but we had to remove something to achieve it.


Plotting the new graph with the removed point.
```{r}
log.df.removed = data.frame(y = test.removed$SalePrice, x = log.pls.pred.out.removed[,1,1])

ggplot(data = log.df.removed, aes(x = x, y = y)) + geom_point() + 
  labs(x = "Predicted Sale Price", y = "Actual Sale Price", title = "Partial Least Squares on Log of Sale Price Model") + 
  geom_abline(slope = 1)
```

Residual plot
```{r}
library(scales)
resids = test.removed$SalePrice - log.pls.pred.out.removed[,1,1]

resids.df = data.frame(x = log.pls.pred.out.removed[,1,1], y = resids)

ggplot(data = resids.df, aes(x = log.pls.pred.out.removed[,1,1], y = resids)) + 
  geom_point() + labs(x = "Predicted Sale Price", y = "Residuals", title = "Partial Least Squares on Log of Sale Price Model") +
  geom_abline(slope = 0) + scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma)
```

